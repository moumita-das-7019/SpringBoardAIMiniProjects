{"cells":[{"metadata":{"hide":true},"cell_type":"markdown","source":"# Classification\n$$\n\\renewcommand{\\like}{{\\cal L}}\n\\renewcommand{\\loglike}{{\\ell}}\n\\renewcommand{\\err}{{\\cal E}}\n\\renewcommand{\\dat}{{\\cal D}}\n\\renewcommand{\\hyp}{{\\cal H}}\n\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n\\renewcommand{\\x}{{\\mathbf x}}\n\\renewcommand{\\v}[1]{{\\mathbf #1}}\n$$"},{"metadata":{},"cell_type":"markdown","source":"**Note:** We've adapted this Mini Project from [Lab 5 in the CS109](https://github.com/cs109/2015lab5) course. Please feel free to check out the original lab, both for more exercises, as well as solutions."},{"metadata":{},"cell_type":"markdown","source":"We turn our attention to **classification**. Classification tries to predict, which of a small set of classes, an observation belongs to. Mathematically, the aim is to find $y$, a **label** based on knowing a feature vector $\\x$. For instance, consider predicting gender from seeing a person's face, something we do fairly well as humans. To have a machine do this well, we would typically feed the machine a bunch of images of people which have been labelled \"male\" or \"female\" (the training set), and have it learn the gender of the person in the image from the labels and the *features* used to determine gender. Then, given a new photo, the trained algorithm returns us the gender of the person in the photo.\n\nThere are different ways of making classifications. One idea is shown schematically in the image below, where we find a line that divides \"things\" of two different types in a 2-dimensional feature space. The classification show in the figure below is an example of a maximum-margin classifier where construct a decision boundary that is far as possible away from both classes of points. The fact that a line can be drawn to separate the two classes makes the problem *linearly separable*. Support Vector Machines (SVM) are an example of a maximum-margin classifier.\n\n![Splitting using a single line](images/onelinesplit.png)\n\n"},{"metadata":{"hide":true,"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\nimport sklearn.model_selection\nimport seaborn as sns\n\nc0=sns.color_palette()[0]\nc1=sns.color_palette()[1]\nc2=sns.color_palette()[2]\n\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\ncm = plt.cm.RdBu\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\n\ndef points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, \n                cdiscrete=cmap_bold, alpha=0.1, psize=10, zfunc=False, predicted=False):\n    h = .02\n    X=np.concatenate((Xtr, Xte))\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n\n    #plt.figure(figsize=(10,6))\n    if zfunc:\n        p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n        p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n        Z=zfunc(p0, p1)\n    else:\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    ZZ = Z.reshape(xx.shape)\n    if mesh:\n        plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light, alpha=alpha, axes=ax)\n    if predicted:\n        showtr = clf.predict(Xtr)\n        showte = clf.predict(Xte)\n    else:\n        showtr = ytr\n        showte = yte\n    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=showtr-1, cmap=cmap_bold, \n               s=psize, alpha=alpha,edgecolor=\"k\")\n    # and testing points\n    ax.scatter(Xte[:, 0], Xte[:, 1], c=showte-1, cmap=cmap_bold, \n               alpha=alpha, marker=\"s\", s=psize+10)\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    return ax,xx,yy\n\ndef points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, \n                     cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1):\n    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, \n                           colorscale=colorscale, cdiscrete=cdiscrete, \n                           psize=psize, alpha=alpha, predicted=True) \n    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14, axes=ax)\n    return ax ","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A Motivating Example Using `sklearn`: Heights and Weights"},{"metadata":{},"cell_type":"markdown","source":"We'll use a dataset of heights and weights of males and females to hone our understanding of classifiers. We load the data into a dataframe and plot it."},{"metadata":{"trusted":true},"cell_type":"code","source":"dflog = pd.read_csv(\"../input/815logical-regression/01_heights_weights_genders.csv\")\ndflog.head()","execution_count":4,"outputs":[{"output_type":"error","ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-9145e9ee6bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdflog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/815logical-regression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdflog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."]}]},{"metadata":{},"cell_type":"markdown","source":"Remember that the form of data we will use always is\n\n![dataform](images/data.png)\n\nwith the \"response\" or \"label\" $y$ as a plain array of 0s and 1s for binary classification. Sometimes we will also see -1 and +1 instead. There are also *multiclass* classifiers that can assign an observation to one of $K > 2$ classes and the label may then be an integer, but we will not be discussing those here.\n\n`y = [1,1,0,0,0,1,0,1,0....]`."},{"metadata":{},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<h3>Checkup Exercise Set I</h3>\n\n<ul>\n  <li> <b>Exercise:</b> Create a scatter plot of Weight vs. Height\n  <li> <b>Exercise:</b> Color the points differently by Gender\n</ul>\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# your turn\nsns.lmplot(x='Height',y='Weight',hue='Gender', data=dflog, fit_reg = False, scatter_kws={'s': 10})\nplt.title('Weight vs. Height for males and females')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training and Test Datasets\n\nWhen fitting models, we would like to ensure two things:\n\n* We have found the best model (in terms of model parameters).\n* The model is highly likely to generalize i.e. perform well on unseen data.\n\n<br/>\n<div class=\"span5 alert alert-success\">\n<h4>Purpose of splitting data into Training/testing sets</h4>\n<ul>\n  <li> We built our model with the requirement that the model fit the data well. </li>\n  <li> As a side-effect, the model will fit <b>THIS</b> dataset well. What about new data? </li>\n    <ul>\n      <li> We wanted the model for predictions, right?</li>\n    </ul>\n  <li> One simple solution, leave out some data (for <b>testing</b>) and <b>train</b> the model on the rest </li>\n  <li> This also leads directly to the idea of cross-validation, next section. </li>  \n</ul>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"First, we try a basic Logistic Regression:\n\n* Split the data into a training and test (hold-out) set\n* Train on the training set, and test for accuracy on the testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Split the data into a training and test set.\nXlr, Xtestlr, ylr, ytestlr = train_test_split(dflog[['Height','Weight']].values, \n                                              (dflog.Gender == \"Male\").values,random_state=5)\n\nclf = LogisticRegression()\n# Fit the model on the trainng data.\nclf.fit(Xlr, ylr)\n# Print the accuracy from the testing data.\nprint(accuracy_score(clf.predict(Xtestlr), ytestlr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tuning the Model"},{"metadata":{},"cell_type":"markdown","source":"The model has some hyperparameters we can tune for hopefully better performance. For tuning the parameters of your model, you will use a mix of *cross-validation* and *grid search*. In Logistic Regression, the most important parameter to tune is the *regularization parameter* `C`. Note that the regularization parameter is not always part of the logistic regression model. \n\nThe regularization parameter is used to control for unlikely high regression coefficients, and in other cases can be used when data is sparse, as a method of feature selection.\n\nYou will now implement some code to perform model tuning and selecting the regularization parameter $C$."},{"metadata":{},"cell_type":"markdown","source":"We use the following `cv_score` function to perform K-fold cross-validation and apply a scoring function to each test fold. In this incarnation we use accuracy score as the default scoring function."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\n\ndef cv_score(clf, x, y, score_func=accuracy_score):\n    result = 0\n    nfold = 5\n    for train, test in KFold(nfold).split(x): # split data into train/test groups, 5 times\n        clf.fit(x[train], y[train]) # fit\n        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data\n    return result / nfold # average","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is an example of using the `cv_score` function for a basic logistic regression model without regularization."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression()\nscore = cv_score(clf, Xlr, ylr)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<h3>Checkup Exercise Set II</h3>\n\n<b>Exercise:</b> Implement the following search procedure to find a good model\n<ul>\n<li> You are given a list of possible values of `C` below\n<li> For each C:\n  <ol>\n  <li> Create a logistic regression model with that value of C\n  <li> Find the average score for this model using the `cv_score` function **only on the training set** `(Xlr, ylr)`\n  </ol>\n<li> Pick the C with the highest average score\n</ul>\nYour goal is to find the best model parameters based *only* on the training set, without showing the model test set at all (which is why the test set is also called a *hold-out* set).\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#the grid of parameters to search over\nCs = [0.001, 0.1, 1, 10, 100]\n\nfor c in Cs:\n    clf = LogisticRegression(C=c)\n    score = cv_score(clf,Xlr, ylr)\n    print('Regularization strength: {0}, Score: {1}'.format(str(c), str(score)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Based on the above results, it seems that the model performs the same on test data with or without regularization"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<h3>Checkup Exercise Set III</h3>\n**Exercise:** Now you want to estimate how this model will predict on unseen data in the following way:\n<ol>\n<li> Use the C you obtained from the procedure earlier and train a Logistic Regression on the training data\n<li> Calculate the accuracy on the test data\n</ol>\n\n<p>You may notice that this particular value of `C` may or may not do as well as simply running the default model on a random train-test split. </p>\n\n<ul>\n<li> Do you think that's a problem? \n<li> Why do we need to do this whole cross-validation and grid search stuff anyway?\n</ul>\n\n</div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# your turn\nclf = LogisticRegression()\nscore = cv_score(clf, Xtestlr, ytestlr)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The test data actually outperformed the trainning data without any regularization, which means this model generalized really well.  That is great, and probably due to the small number features (compaired to number of samples).  However, when models have much larger feature sets and(or) a smaller number of samples, model generalization will suffer.  In those cases, regularization will have a larger effect on testing performance."},{"metadata":{},"cell_type":"markdown","source":"### Black Box Grid Search in `sklearn`"},{"metadata":{},"cell_type":"markdown","source":"Scikit-learn, as with many other Python packages, provides utilities to perform common operations so you do not have to do it manually. It is important to understand the mechanics of each operation, but at a certain point, you will want to use the utility instead to save time..."},{"metadata":{},"cell_type":"markdown","source":"<div class=\"span5 alert alert-info\">\n<h3>Checkup Exercise Set IV</h3>\n\n<b>Exercise:</b> Use scikit-learn's [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) tool to perform cross validation and grid search. \n\n* Instead of writing your own loops above to iterate over the model parameters, can you use GridSearchCV to find the best model over the training set? \n* Does it give you the same best value of `C`?\n* How does this model you've obtained perform on the test set?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports for GridSearchCV\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# your turn\nparameters = {'C':[0.001, 0.1, 1, 10, 100]}\nlogR = LogisticRegression()\nclf = GridSearchCV(logR, param_grid=parameters, cv=5)\nclf.fit(Xlr,ylr)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Above are the optimized parameters generated by GridSearchCV.  The regularization value of C, is the same as I found earlier in Checkup Exercise Set II"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_accuracy = clf.score(Xlr, ylr)\ntest_accuracy = clf.score(Xtestlr, ytestlr)\nprint('Training score: {0}, Test score: {1}'.format(str(training_accuracy), str(test_accuracy)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The above trainning score is slightly less than I found earlier, but the test score is the same, so I would say GridSearchCV generated an equivilent model to the one in Checkup Exercise Set II"},{"metadata":{},"cell_type":"markdown","source":"## A Walkthrough of the Math Behind Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"### Setting up Some Demo Code"},{"metadata":{},"cell_type":"markdown","source":"Let's first set some code up for classification that we will need for further discussion on the math. We first set up a function `cv_optimize` which takes a classifier `clf`, a grid of hyperparameters (such as a complexity parameter or regularization parameter) implemented as a dictionary `parameters`, a training set (as a samples x features array) `Xtrain`, and a set of labels `ytrain`. The code takes the traning set, splits it into `n_folds` parts, sets up `n_folds` folds, and carries out a cross-validation by splitting the training set into a training and validation section for each foldfor us. It prints the best value of the parameters, and retuens the best classifier to us."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=5):\n    gs = sklearn.model_selection.GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n    gs.fit(Xtrain, ytrain)\n    print(\"BEST PARAMS\", gs.best_params_)\n    best = gs.best_estimator_\n    return best","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We then use this best classifier to fit the entire training set. This is done inside the `do_classify` function which takes a dataframe `indf` as input. It takes the columns in the list `featurenames` as the features used to train the classifier. The column `targetname` sets the target. The classification is done by setting those samples for which `targetname` has value `target1val` to the value 1, and all others to 0. We split the dataframe into 80% training and 20% testing by default, standardizing the dataset if desired. (Standardizing a data set involves scaling the data so that it has 0 mean and is described in units of its standard deviation. We then train the model on the training set using cross-validation. Having obtained the best classifier using `cv_optimize`, we retrain on the entire training set and calculate the training and testing accuracy, which we print. We return the split data and the trained classifier."},{"metadata":{"hide":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef do_classify(clf, parameters, indf, featurenames, targetname, target1val, standardize=False, train_size=0.8):\n    subdf=indf[featurenames]\n    if standardize:\n        subdfstd=(subdf - subdf.mean())/subdf.std()\n    else:\n        subdfstd=subdf\n    X=subdfstd.values\n    y=(indf[targetname].values==target1val)*1\n    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size)\n    clf = cv_optimize(clf, parameters, Xtrain, ytrain)\n    clf=clf.fit(Xtrain, ytrain)\n    training_accuracy = clf.score(Xtrain, ytrain)\n    test_accuracy = clf.score(Xtest, ytest)\n    print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n    print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n    return clf, Xtrain, ytrain, Xtest, ytest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression: The Math"},{"metadata":{},"cell_type":"markdown","source":"We could approach classification as linear regression, there the class, 0 or 1, is the target variable $y$. But this ignores the fact that our output $y$ is discrete valued, and futhermore, the $y$ predicted by linear regression will in general take on values less than 0 and greater than 1. Additionally, the residuals from the linear regression model will *not* be normally distributed. This violation means we should not use linear regression.\n\nBut what if we could change the form of our hypotheses $h(x)$ instead?\n\nThe idea behind logistic regression is very simple. We want to draw a line in feature space that divides the '1' samples from the '0' samples, just like in the diagram above. In other words, we wish to find the \"regression\" line which divides the samples. Now, a line has the form $w_1 x_1 + w_2 x_2 + w_0 = 0$ in 2-dimensions. On one side of this line we have \n\n$$w_1 x_1 + w_2 x_2 + w_0 \\ge 0,$$\n\nand on the other side we have \n\n$$w_1 x_1 + w_2 x_2 + w_0 < 0.$$ \n\nOur classification rule then becomes:\n\n\\begin{eqnarray*}\ny = 1 &\\mbox{if}& \\v{w}\\cdot\\v{x} \\ge 0\\\\\ny = 0 &\\mbox{if}& \\v{w}\\cdot\\v{x} < 0\n\\end{eqnarray*}\n\nwhere $\\v{x}$ is the vector $\\{1,x_1, x_2,...,x_n\\}$ where we have also generalized to more than 2 features.\n\nWhat hypotheses $h$ can we use to achieve this? One way to do so is to use the **sigmoid** function:\n\n$$h(z) = \\frac{1}{1 + e^{-z}}.$$\n\nNotice that at $z=0$ this function has the value 0.5. If $z > 0$, $h > 0.5$ and as $z \\to \\infty$, $h \\to 1$. If $z < 0$, $h < 0.5$ and as $z \\to -\\infty$, $h \\to 0$. As long as we identify any value of $y > 0.5$ as 1, and any $y < 0.5$ as 0, we can achieve what we wished above.\n\nThis function is plotted below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"h = lambda z: 1. / (1 + np.exp(-z))\nzs=np.arange(-5, 5, 0.1)\nplt.plot(zs, h(zs), alpha=0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we then come up with our rule by identifying:\n\n$$z = \\v{w}\\cdot\\v{x}.$$\n\nThen $h(\\v{w}\\cdot\\v{x}) \\ge 0.5$ if $\\v{w}\\cdot\\v{x} \\ge 0$ and $h(\\v{w}\\cdot\\v{x}) \\lt 0.5$ if $\\v{w}\\cdot\\v{x} \\lt 0$, and:\n\n\\begin{eqnarray*}\ny = 1 &if& h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\\\\ny = 0 &if& h(\\v{w}\\cdot\\v{x}) \\lt 0.5.\n\\end{eqnarray*}\n\nWe will show soon that this identification can be achieved by minimizing a loss in the ERM framework called the **log loss** :\n\n$$ R_{\\cal{D}}(\\v{w}) = - \\sum_{y_i \\in \\cal{D}} \\left ( y_i \\log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) \\log(1 - h(\\v{w}\\cdot\\v{x})) \\right )$$\n\nWe will also add a regularization term:\n\n$$ R_{\\cal{D}}(\\v{w}) = - \\sum_{y_i \\in \\cal{D}} \\left ( y_i \\log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) \\log(1 - h(\\v{w}\\cdot\\v{x})) \\right ) + \\frac{1}{C} \\v{w}\\cdot\\v{w},$$\n\nwhere $C$ is the regularization strength (equivalent to $1/\\alpha$ from the Ridge case), and smaller values of $C$ mean stronger regularization. As before, the regularization tries to prevent features from having terribly high weights, thus implementing a form of feature selection. \n\nHow did we come up with this loss? We'll come back to that, but let us see how logistic regression works out. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"dflog.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_l, Xtrain_l, ytrain_l, Xtest_l, ytest_l  = do_classify(LogisticRegression(), \n                                                           {\"C\": [0.01, 0.1, 1, 10, 100]}, \n                                                           dflog, ['Weight', 'Height'], 'Gender','Male')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nax=plt.gca()\npoints_plot(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, alpha=0.2);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the figure here showing the results of the logistic regression, we plot the actual labels of both the training(circles) and test(squares) samples. The 0's (females) are plotted in red, the 1's (males) in blue. We also show the classification boundary, a line (to the resolution of a grid square). Every sample on the red background side of the line will be classified female, and every sample on the blue side, male. Notice that most of the samples are classified well, but there are misclassified people on both sides, as evidenced by leakage of dots or squares of one color ontothe side of the other color. Both test and traing accuracy are about 92%."},{"metadata":{},"cell_type":"markdown","source":"### The Probabilistic Interpretaion"},{"metadata":{},"cell_type":"markdown","source":"Remember we said earlier that if $h > 0.5$ we ought to identify the sample with $y=1$? One way of thinking about this is to identify $h(\\v{w}\\cdot\\v{x})$ with the probability that the sample is a '1' ($y=1$). Then we have the intuitive notion that lets identify a sample as 1 if we find that the probabilty of being a '1' is $\\ge 0.5$.\n\nSo suppose we say then that the probability of $y=1$ for a given $\\v{x}$ is given by $h(\\v{w}\\cdot\\v{x})$?\n\nThen, the conditional probabilities of $y=1$ or $y=0$ given a particular sample's features $\\v{x}$ are:\n\n\\begin{eqnarray*}\nP(y=1 | \\v{x}) &=& h(\\v{w}\\cdot\\v{x}) \\\\\nP(y=0 | \\v{x}) &=& 1 - h(\\v{w}\\cdot\\v{x}).\n\\end{eqnarray*}\n\nThese two can be written together as\n\n$$P(y|\\v{x}, \\v{w}) = h(\\v{w}\\cdot\\v{x})^y \\left(1 - h(\\v{w}\\cdot\\v{x}) \\right)^{(1-y)} $$\n\nThen multiplying over the samples we get the probability of the training $y$ given $\\v{w}$ and the $\\v{x}$:\n\n$$P(y|\\v{x},\\v{w}) = P(\\{y_i\\} | \\{\\v{x}_i\\}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} P(y_i|\\v{x_i}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}$$\n\nWhy use probabilities? Earlier, we talked about how the regression function $f(x)$ never gives us the $y$ exactly, because of noise. This hold for classification too. Even with identical features, a different sample may be classified differently. \n\nWe said that another way to think about a noisy $y$ is to imagine that our data $\\dat$ was generated from  a joint probability distribution $P(x,y)$. Thus we need to model $y$ at a given $x$, written as $P(y|x)$, and since $P(x)$ is also a probability distribution, we have:\n\n$$P(x,y) = P(y | x) P(x)$$\n\nand can obtain our joint probability $P(x, y)$.\n\nIndeed its important to realize that a particular training set can be thought of as a draw from some \"true\" probability distribution (just as we did when showing the hairy variance diagram). If for example the probability of classifying a test sample as a '0' was 0.1, and it turns out that the test sample was a '0', it does not mean that this model was necessarily wrong. After all, in roughly a 10th of the draws, this new sample would be classified as a '0'! But, of-course its more unlikely than its likely, and having good probabilities means that we'll be likely right most of the time, which is what we want to achieve in classification. And furthermore, we can quantify this accuracy.\n\nThus its desirable to have probabilistic, or at the very least, ranked models of classification where you can tell which sample is more likely to be classified as a '1'. There are business reasons for this too. Consider the example of customer \"churn\": you are a cell-phone company and want to know, based on some of my purchasing habit and characteristic \"features\" if I am a likely defector. If so, you'll offer me an incentive not to defect. In this scenario, you might want to know which customers are most likely to defect, or even more precisely, which are most likely to respond to incentives. Based on these probabilities, you could then spend a finite marketing budget wisely."},{"metadata":{},"cell_type":"markdown","source":"### Maximizing the Probability of the Training Set"},{"metadata":{},"cell_type":"markdown","source":"Now if we maximize $P(y|\\v{x},\\v{w})$, we will maximize the chance that each point is classified correctly, which is what we want to do. While this is not exactly the same thing as maximizing the 1-0 training risk, it is a principled way of obtaining the highest probability classification. This process is called **maximum likelihood** estimation since we are maximising the **likelihood of the training data y**, \n\n$$\\like = P(y|\\v{x},\\v{w}).$$ \n\nMaximum likelihood is one of the corenerstone methods in statistics, and is used to estimate probabilities of data. \n\nWe can equivalently maximize \n\n$$\\loglike = \\log{P(y|\\v{x},\\v{w})}$$ \n\nsince the natural logarithm $\\log$ is a monotonic function. This is known as maximizing the **log-likelihood**. Thus we can equivalently *minimize* a risk that is the negative of  $\\log(P(y|\\v{x},\\v{w}))$:\n\n$$R_{\\cal{D}}(h(x)) = -\\loglike = -\\log \\like = -\\log{P(y|\\v{x},\\v{w})}.$$\n\n\nThus\n\n\\begin{eqnarray*}\nR_{\\cal{D}}(h(x)) &=& -\\log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n                  &=& -\\sum_{y_i \\in \\cal{D}} \\log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n                  &=& -\\sum_{y_i \\in \\cal{D}} \\log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + \\log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n                  &=& - \\sum_{y_i \\in \\cal{D}} \\left ( y_i \\log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) \\log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\n\\end{eqnarray*}\n                  \nThis is exactly the risk we had above, leaving out the regularization term (which we shall return to later) and was the reason we chose it over the 1-0 risk. \n\nNotice that this little process we carried out above tells us something very interesting: **Probabilistic estimation using maximum likelihood is equivalent to Empiricial Risk Minimization using the negative log-likelihood**, since all we did was to minimize the negative log-likelihood over the training samples.\n\n`sklearn` will return the probabilities for our samples, or for that matter, for any input vector set $\\{\\v{x}_i\\}$, i.e. $P(y_i | \\v{x}_i, \\v{w})$:"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_l.predict_proba(Xtest_l)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Discriminative vs Generative Classifier"},{"metadata":{},"cell_type":"markdown","source":"Logistic regression is what is known as a **discriminative classifier** as we learn a soft boundary between/among classes. Another paradigm is the **generative classifier** where we learn the distribution of each class. For more examples of generative classifiers, look [here](https://en.wikipedia.org/wiki/Generative_model). \n\nLet us plot the probabilities obtained from `predict_proba`, overlayed on the samples with their true labels:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nax = plt.gca()\npoints_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, psize=20, alpha=0.1);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that lines of equal probability, as might be expected are stright lines. What the classifier does is very intuitive: if the probability is greater than 0.5, it classifies the sample as type '1' (male), otherwise it classifies the sample to be class '0'. Thus in the diagram above, where we have plotted predicted values rather than actual labels of samples, there is a clear demarcation at the 0.5 probability line.\n\nAgain, this notion of trying to obtain the line or boundary of demarcation is what is called a **discriminative** classifier. The algorithm tries to find a decision boundary that separates the males from the females. To classify a new sample as male or female, it checks on which side of the decision boundary the sample falls, and makes a prediction. In other words we are asking, given $\\v{x}$, what is the probability of a given $y$, or, what is the likelihood $P(y|\\v{x},\\v{w})$?"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [conda env:py36]","language":"python","name":"conda-env-py36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}